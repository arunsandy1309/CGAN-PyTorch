{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea2ab7c",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adb38d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd.variable import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import imageio\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d674a83",
   "metadata": {},
   "source": [
    "## Loading the Rock/Paper/Scissor Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5569b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])])\n",
    "train_dataset = datasets.ImageFolder(root='./data/rps', transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f932c",
   "metadata": {},
   "source": [
    "## Initializing the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51180ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d839d0",
   "metadata": {},
   "source": [
    "## Creating the Generator & Discriminator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0ae616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "         \n",
    "      \n",
    "        self.label_conditioned_generator =nn.Sequential(nn.Embedding(3, 100),\n",
    "                      nn.Linear(100, 16))\n",
    "         \n",
    "     \n",
    "        self.latent =nn.Sequential(nn.Linear(100, 4*4*512),\n",
    "                                   nn.LeakyReLU(0.2, inplace=True))\n",
    "            \n",
    " \n",
    "        self.model =nn.Sequential(nn.ConvTranspose2d(513, 64*8, 4, 2, 1, bias=False),\n",
    "                      nn.BatchNorm2d(64*8, momentum=0.1,  eps=0.8),\n",
    "                      nn.ReLU(True),\n",
    "                      nn.ConvTranspose2d(64*8, 64*4, 4, 2, 1,bias=False),\n",
    "                      nn.BatchNorm2d(64*4, momentum=0.1,  eps=0.8),\n",
    "                      nn.ReLU(True), \n",
    "                      nn.ConvTranspose2d(64*4, 64*2, 4, 2, 1,bias=False),\n",
    "                      nn.BatchNorm2d(64*2, momentum=0.1,  eps=0.8),\n",
    "                      nn.ReLU(True), \n",
    "                      nn.ConvTranspose2d(64*2, 64*1, 4, 2, 1,bias=False),\n",
    "                      nn.BatchNorm2d(64*1, momentum=0.1,  eps=0.8),\n",
    "                      nn.ReLU(True), \n",
    "                      nn.ConvTranspose2d(64*1, 3, 4, 2, 1, bias=False),\n",
    "                      nn.Tanh())\n",
    " \n",
    "    def forward(self, inputs):\n",
    "        noise_vector, label = inputs\n",
    "        label_output = self.label_conditioned_generator(label)\n",
    "        label_output = label_output.view(-1, 1, 4, 4)\n",
    "        latent_output = self.latent(noise_vector)\n",
    "        latent_output = latent_output.view(-1, 512,4,4)\n",
    "        concat = torch.cat((latent_output, label_output), dim=1)\n",
    "        image = self.model(concat)\n",
    "        #print(image.size())\n",
    "        return image\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "         \n",
    "        self.label_condition_disc = nn.Sequential(nn.Embedding(3, 100),\n",
    "                      nn.Linear(100, 3*128*128))       \n",
    "        \n",
    "        self.model = nn.Sequential(nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "                      nn.LeakyReLU(0.2, inplace=True),\n",
    "                      nn.Conv2d(64, 64*2, 4, 3, 2, bias=False),\n",
    "                      nn.BatchNorm2d(64*2, momentum=0.1,  eps=0.8),\n",
    "                      nn.LeakyReLU(0.2, inplace=True),\n",
    "                      nn.Conv2d(64*2, 64*4, 4, 3,2, bias=False),\n",
    "                      nn.BatchNorm2d(64*4, momentum=0.1,  eps=0.8),\n",
    "                      nn.LeakyReLU(0.2, inplace=True),\n",
    "                      nn.Conv2d(64*4, 64*8, 4, 3, 2, bias=False),\n",
    "                      nn.BatchNorm2d(64*8, momentum=0.1,  eps=0.8),\n",
    "                      nn.LeakyReLU(0.2, inplace=True), \n",
    "                      nn.Flatten(),\n",
    "                      nn.Dropout(0.4),\n",
    "                      nn.Linear(9216, 1),\n",
    "                      nn.Sigmoid()\n",
    "                     )\n",
    " \n",
    "    def forward(self, inputs):\n",
    "        img, label = inputs\n",
    "        label_output = self.label_condition_disc(label)\n",
    "        label_output = label_output.view(-1, 3, 128, 128)\n",
    "        concat = torch.cat((img, label_output), dim=-1)\n",
    "        #print(concat.size())\n",
    "        output = self.model(concat)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c3aa16",
   "metadata": {},
   "source": [
    "## Creating Objects, Optimizer, BCE Loss & Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1314b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(n, n_features=128):\n",
    "    return Variable(torch.randn(n, n_features)).to(device)\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "generator = Generator().to(device)\n",
    "generator.apply(weights_init)\n",
    "discriminator = Discriminator().to(device)\n",
    "discriminator.apply(weights_init)\n",
    "\n",
    "learning_rate = 0.0002\n",
    "G_optimizer = optim.Adam(generator.parameters(), lr = learning_rate, betas=(0.5, 0.999))\n",
    "D_optimizer = optim.Adam(discriminator.parameters(), lr = learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "images = []\n",
    "test_noise = noise(100)\n",
    "\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406362a5",
   "metadata": {},
   "source": [
    "## Calling the BCE Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37f63d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output, label):\n",
    "    gen_loss = adversarial_loss(fake_output, label)\n",
    "    #print(gen_loss)\n",
    "    return gen_loss\n",
    "\n",
    "def discriminator_loss(output, label):\n",
    "    disc_loss = adversarial_loss(output, label)\n",
    "    return disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c09440",
   "metadata": {},
   "source": [
    "## Training the Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b398ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\n",
      "Epoch 1: g_loss: 2.54973888 d_loss: 0.30868986\n",
      "Epoch : 2\n",
      "Epoch 2: g_loss: 6.43261433 d_loss: 0.01482268\n",
      "Epoch : 3\n",
      "Epoch 3: g_loss: 10.39438534 d_loss: 0.11885827\n",
      "Epoch : 4\n",
      "Epoch 4: g_loss: 4.58300400 d_loss: 0.40138936\n",
      "Epoch : 5\n",
      "Epoch 5: g_loss: 3.26647854 d_loss: 0.33969256\n",
      "Epoch : 6\n",
      "Epoch 6: g_loss: 3.87531495 d_loss: 0.23192680\n",
      "Epoch : 7\n",
      "Epoch 7: g_loss: 4.78416252 d_loss: 0.25680047\n",
      "Epoch : 8\n",
      "Epoch 8: g_loss: 5.04929399 d_loss: 0.54201931\n",
      "Epoch : 9\n",
      "Epoch 9: g_loss: 3.38708711 d_loss: 0.33442551\n",
      "Epoch : 10\n",
      "Epoch 10: g_loss: 5.02778339 d_loss: 0.42029729\n",
      "Epoch : 11\n",
      "Epoch 11: g_loss: 3.41607261 d_loss: 0.43709859\n",
      "Epoch : 12\n",
      "Epoch 12: g_loss: 2.70365405 d_loss: 0.30947712\n",
      "Epoch : 13\n",
      "Epoch 13: g_loss: 2.88833117 d_loss: 0.56385010\n",
      "Epoch : 14\n",
      "Epoch 14: g_loss: 2.27438402 d_loss: 0.49697697\n",
      "Epoch : 15\n",
      "Epoch 15: g_loss: 4.06546736 d_loss: 0.51908034\n",
      "Epoch : 16\n",
      "Epoch 16: g_loss: 3.60778427 d_loss: 0.32007274\n",
      "Epoch : 17\n",
      "Epoch 17: g_loss: 3.14119387 d_loss: 0.61742479\n",
      "Epoch : 18\n",
      "Epoch 18: g_loss: 2.32913232 d_loss: 0.41705090\n",
      "Epoch : 19\n",
      "Epoch 19: g_loss: 2.02217937 d_loss: 0.51203734\n",
      "Epoch : 20\n",
      "Epoch 20: g_loss: 2.07838273 d_loss: 0.43555856\n",
      "Epoch : 21\n",
      "Epoch 21: g_loss: 2.37276602 d_loss: 0.40659925\n",
      "Epoch : 22\n",
      "Epoch 22: g_loss: 2.35010743 d_loss: 0.30305052\n",
      "Epoch : 23\n",
      "Epoch 23: g_loss: 2.36156678 d_loss: 0.43750453\n",
      "Epoch : 24\n",
      "Epoch 24: g_loss: 3.40735745 d_loss: 0.39825758\n",
      "Epoch : 25\n",
      "Epoch 25: g_loss: 2.68832827 d_loss: 0.53970015\n",
      "Epoch : 26\n",
      "Epoch 26: g_loss: 2.66172266 d_loss: 0.37947202\n",
      "Epoch : 27\n",
      "Epoch 27: g_loss: 2.58291960 d_loss: 0.45408732\n",
      "Epoch : 28\n",
      "Epoch 28: g_loss: 2.49831605 d_loss: 0.39160123\n",
      "Epoch : 29\n",
      "Epoch 29: g_loss: 2.82541275 d_loss: 0.38201103\n",
      "Epoch : 30\n",
      "Epoch 30: g_loss: 2.75930023 d_loss: 0.47311133\n",
      "Epoch : 31\n",
      "Epoch 31: g_loss: 2.11673212 d_loss: 0.44830802\n",
      "Epoch : 32\n",
      "Epoch 32: g_loss: 2.45457029 d_loss: 0.40538412\n",
      "Epoch : 33\n",
      "Epoch 33: g_loss: 1.98195720 d_loss: 0.47758693\n",
      "Epoch : 34\n",
      "Epoch 34: g_loss: 1.90818024 d_loss: 0.50433880\n",
      "Epoch : 35\n",
      "Epoch 35: g_loss: 1.89240134 d_loss: 0.45126989\n",
      "Epoch : 36\n",
      "Epoch 36: g_loss: 2.01677728 d_loss: 0.54657680\n",
      "Epoch : 37\n",
      "Epoch 37: g_loss: 1.60298908 d_loss: 0.49095446\n",
      "Epoch : 38\n",
      "Epoch 38: g_loss: 1.67290938 d_loss: 0.49335805\n",
      "Epoch : 39\n",
      "Epoch 39: g_loss: 1.60688698 d_loss: 0.55143303\n",
      "Epoch : 40\n",
      "Epoch 40: g_loss: 1.49695766 d_loss: 0.59408319\n",
      "Epoch : 41\n",
      "Epoch 41: g_loss: 1.41362035 d_loss: 0.54483694\n",
      "Epoch : 42\n",
      "Epoch 42: g_loss: 1.48551619 d_loss: 0.56529051\n",
      "Epoch : 43\n",
      "Epoch 43: g_loss: 1.37635398 d_loss: 0.54530579\n",
      "Epoch : 44\n",
      "Epoch 44: g_loss: 1.55292237 d_loss: 0.52330548\n",
      "Epoch : 45\n",
      "Epoch 45: g_loss: 1.62371981 d_loss: 0.48978239\n",
      "Epoch : 46\n",
      "Epoch 46: g_loss: 1.74595070 d_loss: 0.51928228\n",
      "Epoch : 47\n",
      "Epoch 47: g_loss: 1.95352733 d_loss: 0.58183438\n",
      "Epoch : 48\n",
      "Epoch 48: g_loss: 1.80907238 d_loss: 0.56110740\n",
      "Epoch : 49\n",
      "Epoch 49: g_loss: 1.73252749 d_loss: 0.55900455\n",
      "Epoch : 50\n",
      "Epoch 50: g_loss: 1.53674757 d_loss: 0.57701606\n",
      "Epoch : 51\n",
      "Epoch 51: g_loss: 1.50310099 d_loss: 0.52662480\n",
      "Epoch : 52\n",
      "Epoch 52: g_loss: 1.46217549 d_loss: 0.55562615\n",
      "Epoch : 53\n",
      "Epoch 53: g_loss: 1.39844847 d_loss: 0.54493600\n",
      "Epoch : 54\n",
      "Epoch 54: g_loss: 1.39932275 d_loss: 0.56561142\n",
      "Epoch : 55\n",
      "Epoch 55: g_loss: 1.40422392 d_loss: 0.56384385\n",
      "Epoch : 56\n",
      "Epoch 56: g_loss: 1.37106919 d_loss: 0.57126993\n",
      "Epoch : 57\n",
      "Epoch 57: g_loss: 1.40612161 d_loss: 0.57330990\n",
      "Epoch : 58\n",
      "Epoch 58: g_loss: 1.51596200 d_loss: 0.54430854\n",
      "Epoch : 59\n",
      "Epoch 59: g_loss: 1.59035110 d_loss: 0.55919504\n",
      "Epoch : 60\n",
      "Epoch 60: g_loss: 1.47820210 d_loss: 0.57496399\n",
      "Epoch : 61\n",
      "Epoch 61: g_loss: 1.44520676 d_loss: 0.52458316\n",
      "Epoch : 62\n",
      "Epoch 62: g_loss: 1.48527133 d_loss: 0.56208199\n",
      "Epoch : 63\n",
      "Epoch 63: g_loss: 1.57778454 d_loss: 0.56962234\n",
      "Epoch : 64\n",
      "Epoch 64: g_loss: 1.64143384 d_loss: 0.56131852\n",
      "Epoch : 65\n",
      "Epoch 65: g_loss: 1.65087235 d_loss: 0.57844347\n",
      "Epoch : 66\n",
      "Epoch 66: g_loss: 1.49293542 d_loss: 0.54330337\n",
      "Epoch : 67\n",
      "Epoch 67: g_loss: 1.80062830 d_loss: 0.52772319\n",
      "Epoch : 68\n",
      "Epoch 68: g_loss: 1.91029894 d_loss: 0.59306842\n",
      "Epoch : 69\n",
      "Epoch 69: g_loss: 1.65934110 d_loss: 0.57817197\n",
      "Epoch : 70\n",
      "Epoch 70: g_loss: 1.51045716 d_loss: 0.51612025\n",
      "Epoch : 71\n",
      "Epoch 71: g_loss: 1.56803155 d_loss: 0.55906397\n",
      "Epoch : 72\n",
      "Epoch 72: g_loss: 1.88857985 d_loss: 0.48374093\n",
      "Epoch : 73\n",
      "Epoch 73: g_loss: 1.85684514 d_loss: 0.51481259\n",
      "Epoch : 74\n",
      "Epoch 74: g_loss: 1.74682820 d_loss: 0.51355118\n",
      "Epoch : 75\n",
      "Epoch 75: g_loss: 1.77164829 d_loss: 0.53666341\n",
      "Epoch : 76\n",
      "Epoch 76: g_loss: 1.71873701 d_loss: 0.52335930\n",
      "Epoch : 77\n",
      "Epoch 77: g_loss: 1.66025162 d_loss: 0.55585873\n",
      "Epoch : 78\n",
      "Epoch 78: g_loss: 1.64053845 d_loss: 0.49472746\n",
      "Epoch : 79\n",
      "Epoch 79: g_loss: 1.59245610 d_loss: 0.55722827\n",
      "Epoch : 80\n",
      "Epoch 80: g_loss: 1.61682367 d_loss: 0.50846583\n",
      "Epoch : 81\n",
      "Epoch 81: g_loss: 1.79764295 d_loss: 0.56840980\n",
      "Epoch : 82\n",
      "Epoch 82: g_loss: 1.74685991 d_loss: 0.54478222\n",
      "Epoch : 83\n",
      "Epoch 83: g_loss: 1.75380397 d_loss: 0.54813725\n",
      "Epoch : 84\n",
      "Epoch 84: g_loss: 1.71674573 d_loss: 0.55084676\n",
      "Epoch : 85\n",
      "Epoch 85: g_loss: 1.68751967 d_loss: 0.53704900\n",
      "Epoch : 86\n",
      "Epoch 86: g_loss: 1.68255591 d_loss: 0.54622144\n",
      "Epoch : 87\n",
      "Epoch 87: g_loss: 1.70515561 d_loss: 0.50894141\n",
      "Epoch : 88\n",
      "Epoch 88: g_loss: 1.68447423 d_loss: 0.55204713\n",
      "Epoch : 89\n",
      "Epoch 89: g_loss: 1.58940685 d_loss: 0.48525396\n",
      "Epoch : 90\n",
      "Epoch 90: g_loss: 1.67116106 d_loss: 0.53094870\n",
      "Epoch : 91\n",
      "Epoch 91: g_loss: 1.54094911 d_loss: 0.53260309\n",
      "Epoch : 92\n",
      "Epoch 92: g_loss: 1.67867661 d_loss: 0.51840383\n",
      "Epoch : 93\n",
      "Epoch 93: g_loss: 1.71834457 d_loss: 0.54507881\n",
      "Epoch : 94\n",
      "Epoch 94: g_loss: 1.53032506 d_loss: 0.53312707\n",
      "Epoch : 95\n",
      "Epoch 95: g_loss: 1.67656672 d_loss: 0.53605056\n",
      "Epoch : 96\n",
      "Epoch 96: g_loss: 1.60980046 d_loss: 0.56022513\n",
      "Epoch : 97\n",
      "Epoch 97: g_loss: 1.71101391 d_loss: 0.51417100\n",
      "Epoch : 98\n",
      "Epoch 98: g_loss: 1.69851840 d_loss: 0.53254199\n",
      "Epoch : 99\n",
      "Epoch 99: g_loss: 1.77412629 d_loss: 0.59099615\n",
      "Epoch : 100\n",
      "Epoch 100: g_loss: 1.87372267 d_loss: 0.51521707\n",
      "Epoch : 101\n",
      "Epoch 101: g_loss: 1.73907578 d_loss: 0.54473054\n",
      "Epoch : 102\n",
      "Epoch 102: g_loss: 1.74421120 d_loss: 0.49458998\n",
      "Epoch : 103\n",
      "Epoch 103: g_loss: 1.78328574 d_loss: 0.56165904\n",
      "Epoch : 104\n",
      "Epoch 104: g_loss: 1.67150402 d_loss: 0.53741354\n",
      "Epoch : 105\n",
      "Epoch 105: g_loss: 1.64063430 d_loss: 0.51564825\n",
      "Epoch : 106\n",
      "Epoch 106: g_loss: 1.69105923 d_loss: 0.51348007\n",
      "Epoch : 107\n",
      "Epoch 107: g_loss: 1.72310722 d_loss: 0.51514721\n",
      "Epoch : 108\n",
      "Epoch 108: g_loss: 1.50107157 d_loss: 0.51230580\n",
      "Epoch : 109\n",
      "Epoch 109: g_loss: 1.74873972 d_loss: 0.54673898\n",
      "Epoch : 110\n",
      "Epoch 110: g_loss: 1.70283782 d_loss: 0.49858391\n",
      "Epoch : 111\n",
      "Epoch 111: g_loss: 1.76763630 d_loss: 0.52353626\n",
      "Epoch : 112\n",
      "Epoch 112: g_loss: 1.87193406 d_loss: 0.52247411\n",
      "Epoch : 113\n",
      "Epoch 113: g_loss: 1.68819797 d_loss: 0.51366091\n",
      "Epoch : 114\n",
      "Epoch 114: g_loss: 2.03549075 d_loss: 0.54191959\n",
      "Epoch : 115\n",
      "Epoch 115: g_loss: 1.91852546 d_loss: 0.52066082\n",
      "Epoch : 116\n",
      "Epoch 116: g_loss: 1.94594121 d_loss: 0.54569125\n",
      "Epoch : 117\n",
      "Epoch 117: g_loss: 1.80180740 d_loss: 0.50271165\n",
      "Epoch : 118\n",
      "Epoch 118: g_loss: 1.69584703 d_loss: 0.49659848\n",
      "Epoch : 119\n",
      "Epoch 119: g_loss: 1.75285399 d_loss: 0.49516982\n",
      "Epoch : 120\n",
      "Epoch 120: g_loss: 1.82993960 d_loss: 0.50427097\n",
      "Epoch : 121\n",
      "Epoch 121: g_loss: 1.79146385 d_loss: 0.49401647\n",
      "Epoch : 122\n",
      "Epoch 122: g_loss: 1.84392059 d_loss: 0.53268355\n",
      "Epoch : 123\n",
      "Epoch 123: g_loss: 2.36075163 d_loss: 0.67613852\n",
      "Epoch : 124\n",
      "Epoch 124: g_loss: 1.80490816 d_loss: 0.49411991\n",
      "Epoch : 125\n",
      "Epoch 125: g_loss: 1.84699464 d_loss: 0.46984965\n",
      "Epoch : 126\n",
      "Epoch 126: g_loss: 1.79701114 d_loss: 0.49744701\n",
      "Epoch : 127\n",
      "Epoch 127: g_loss: 1.84405482 d_loss: 0.45827562\n",
      "Epoch : 128\n",
      "Epoch 128: g_loss: 1.70166910 d_loss: 0.48896623\n",
      "Epoch : 129\n",
      "Epoch 129: g_loss: 1.70432079 d_loss: 0.46373287\n",
      "Epoch : 130\n",
      "Epoch 130: g_loss: 1.72352672 d_loss: 0.49602419\n",
      "Epoch : 131\n",
      "Epoch 131: g_loss: 2.03467321 d_loss: 0.53778917\n",
      "Epoch : 132\n",
      "Epoch 132: g_loss: 1.76865852 d_loss: 0.51427490\n",
      "Epoch : 133\n",
      "Epoch 133: g_loss: 1.74824631 d_loss: 0.47855523\n",
      "Epoch : 134\n",
      "Epoch 134: g_loss: 1.82689011 d_loss: 0.47828197\n",
      "Epoch : 135\n",
      "Epoch 135: g_loss: 1.94352317 d_loss: 0.50978088\n",
      "Epoch : 136\n",
      "Epoch 136: g_loss: 1.77632654 d_loss: 0.47766501\n",
      "Epoch : 137\n",
      "Epoch 137: g_loss: 1.70459092 d_loss: 0.48215026\n",
      "Epoch : 138\n",
      "Epoch 138: g_loss: 1.80813324 d_loss: 0.48784706\n",
      "Epoch : 139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139: g_loss: 1.78549647 d_loss: 0.49017021\n",
      "Epoch : 140\n",
      "Epoch 140: g_loss: 1.83087373 d_loss: 0.48579440\n",
      "Epoch : 141\n",
      "Epoch 141: g_loss: 2.08775997 d_loss: 0.49087232\n",
      "Epoch : 142\n",
      "Epoch 142: g_loss: 1.88530016 d_loss: 0.47320977\n",
      "Epoch : 143\n",
      "Epoch 143: g_loss: 1.76153743 d_loss: 0.47236285\n",
      "Epoch : 144\n",
      "Epoch 144: g_loss: 2.04338264 d_loss: 0.53600895\n",
      "Epoch : 145\n",
      "Epoch 145: g_loss: 1.81437802 d_loss: 0.48033306\n",
      "Epoch : 146\n",
      "Epoch 146: g_loss: 1.91237593 d_loss: 0.48745930\n",
      "Epoch : 147\n",
      "Epoch 147: g_loss: 2.07507992 d_loss: 0.54505014\n",
      "Epoch : 148\n",
      "Epoch 148: g_loss: 1.90643990 d_loss: 0.44887039\n",
      "Epoch : 149\n",
      "Epoch 149: g_loss: 2.23041248 d_loss: 0.56925297\n",
      "Epoch : 150\n",
      "Epoch 150: g_loss: 2.32545400 d_loss: 0.50482839\n",
      "Epoch : 151\n",
      "Epoch 151: g_loss: 1.87914085 d_loss: 0.49598262\n",
      "Epoch : 152\n",
      "Epoch 152: g_loss: 1.82312703 d_loss: 0.48816794\n",
      "Epoch : 153\n",
      "Epoch 153: g_loss: 2.34805536 d_loss: 0.56274807\n",
      "Epoch : 154\n",
      "Epoch 154: g_loss: 1.83344483 d_loss: 0.47811356\n",
      "Epoch : 155\n",
      "Epoch 155: g_loss: 1.87497115 d_loss: 0.47614029\n",
      "Epoch : 156\n",
      "Epoch 156: g_loss: 1.90073466 d_loss: 0.47991735\n",
      "Epoch : 157\n",
      "Epoch 157: g_loss: 1.85557091 d_loss: 0.48178932\n",
      "Epoch : 158\n",
      "Epoch 158: g_loss: 1.72874355 d_loss: 0.48862958\n",
      "Epoch : 159\n",
      "Epoch 159: g_loss: 2.09811187 d_loss: 0.57980055\n",
      "Epoch : 160\n",
      "Epoch 160: g_loss: 1.84946167 d_loss: 0.48143336\n",
      "Epoch : 161\n",
      "Epoch 161: g_loss: 1.78805816 d_loss: 0.49966887\n",
      "Epoch : 162\n",
      "Epoch 162: g_loss: 1.87818503 d_loss: 0.52134669\n",
      "Epoch : 163\n",
      "Epoch 163: g_loss: 1.78171647 d_loss: 0.48867989\n",
      "Epoch : 164\n",
      "Epoch 164: g_loss: 1.69272447 d_loss: 0.49339491\n",
      "Epoch : 165\n",
      "Epoch 165: g_loss: 1.66278005 d_loss: 0.50621629\n",
      "Epoch : 166\n",
      "Epoch 166: g_loss: 1.83151126 d_loss: 0.56107432\n",
      "Epoch : 167\n",
      "Epoch 167: g_loss: 2.29350829 d_loss: 0.58287203\n",
      "Epoch : 168\n",
      "Epoch 168: g_loss: 1.97167766 d_loss: 0.48871785\n",
      "Epoch : 169\n",
      "Epoch 169: g_loss: 1.86789048 d_loss: 0.47467822\n",
      "Epoch : 170\n",
      "Epoch 170: g_loss: 1.82031691 d_loss: 0.48314509\n",
      "Epoch : 171\n",
      "Epoch 171: g_loss: 1.86444318 d_loss: 0.51007110\n",
      "Epoch : 172\n",
      "Epoch 172: g_loss: 1.95370150 d_loss: 0.45423979\n",
      "Epoch : 173\n",
      "Epoch 173: g_loss: 1.85845959 d_loss: 0.46693447\n",
      "Epoch : 174\n",
      "Epoch 174: g_loss: 1.81191552 d_loss: 0.46073776\n",
      "Epoch : 175\n",
      "Epoch 175: g_loss: 1.86575854 d_loss: 0.46944365\n",
      "Epoch : 176\n",
      "Epoch 176: g_loss: 1.99267220 d_loss: 0.55729276\n",
      "Epoch : 177\n",
      "Epoch 177: g_loss: 2.03168678 d_loss: 0.46463731\n",
      "Epoch : 178\n",
      "Epoch 178: g_loss: 2.03323603 d_loss: 0.49839771\n",
      "Epoch : 179\n",
      "Epoch 179: g_loss: 1.96725667 d_loss: 0.54223424\n",
      "Epoch : 180\n",
      "Epoch 180: g_loss: 1.86314237 d_loss: 0.49203268\n",
      "Epoch : 181\n",
      "Epoch 181: g_loss: 1.79216182 d_loss: 0.49492398\n",
      "Epoch : 182\n",
      "Epoch 182: g_loss: 1.88793099 d_loss: 0.49116045\n",
      "Epoch : 183\n",
      "Epoch 183: g_loss: 1.85433757 d_loss: 0.46019766\n",
      "Epoch : 184\n",
      "Epoch 184: g_loss: 1.76995063 d_loss: 0.49181393\n",
      "Epoch : 185\n",
      "Epoch 185: g_loss: 1.91900182 d_loss: 0.54943627\n",
      "Epoch : 186\n",
      "Epoch 186: g_loss: 2.10913324 d_loss: 0.54985785\n",
      "Epoch : 187\n",
      "Epoch 187: g_loss: 1.94133604 d_loss: 0.45759472\n",
      "Epoch : 188\n",
      "Epoch 188: g_loss: 1.86178982 d_loss: 0.50166517\n",
      "Epoch : 189\n",
      "Epoch 189: g_loss: 1.91673350 d_loss: 0.46859846\n",
      "Epoch : 190\n",
      "Epoch 190: g_loss: 1.88756454 d_loss: 0.53141505\n",
      "Epoch : 191\n",
      "Epoch 191: g_loss: 2.00561047 d_loss: 0.48447868\n",
      "Epoch : 192\n",
      "Epoch 192: g_loss: 1.86458683 d_loss: 0.47747612\n",
      "Epoch : 193\n",
      "Epoch 193: g_loss: 1.76820469 d_loss: 0.46360829\n",
      "Epoch : 194\n",
      "Epoch 194: g_loss: 1.87079787 d_loss: 0.48572949\n",
      "Epoch : 195\n",
      "Epoch 195: g_loss: 1.87326396 d_loss: 0.48540476\n",
      "Epoch : 196\n",
      "Epoch 196: g_loss: 1.84147775 d_loss: 0.51530164\n",
      "Epoch : 197\n",
      "Epoch 197: g_loss: 1.93985975 d_loss: 0.51906133\n",
      "Epoch : 198\n",
      "Epoch 198: g_loss: 1.93017197 d_loss: 0.49919206\n",
      "Epoch : 199\n",
      "Epoch 199: g_loss: 1.90891111 d_loss: 0.49371573\n",
      "Epoch : 200\n",
      "Epoch 200: g_loss: 1.86578059 d_loss: 0.46602777\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "for epoch in range(1, num_epochs+1): \n",
    "    D_loss_list, G_loss_list = [], []\n",
    "    g_error, d_error=0.0, 0.0\n",
    "    print(\"Epoch :\",epoch)\n",
    "    for index, (real_images, labels) in enumerate(train_loader):\n",
    "        D_optimizer.zero_grad()\n",
    "        real_images = real_images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels.unsqueeze(1).long()\n",
    " \n",
    "       \n",
    "        real_target = Variable(torch.ones(real_images.size(0), 1).to(device))\n",
    "        fake_target = Variable(torch.zeros(real_images.size(0), 1).to(device))\n",
    "       \n",
    "        D_real_loss = discriminator_loss(discriminator((real_images, labels)), real_target)\n",
    "        # print(discriminator(real_images))\n",
    "        #D_real_loss.backward()\n",
    "     \n",
    "        noise_vector = torch.randn(real_images.size(0), 100, device=device)  \n",
    "        noise_vector = noise_vector.to(device)\n",
    "         \n",
    "        \n",
    "        generated_image = generator((noise_vector, labels))\n",
    "        output = discriminator((generated_image.detach(), labels))\n",
    "        D_fake_loss = discriminator_loss(output,  fake_target)\n",
    " \n",
    "     \n",
    "        # train with fake\n",
    "        #D_fake_loss.backward()\n",
    "       \n",
    "        D_total_loss = (D_real_loss + D_fake_loss) / 2\n",
    "        D_loss_list.append(D_total_loss)\n",
    "       \n",
    "        D_total_loss.backward()\n",
    "        D_optimizer.step()\n",
    " \n",
    "        # Train generator with real labels\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss = generator_loss(discriminator((generated_image, labels)), real_target)\n",
    "        G_loss_list.append(G_loss)\n",
    " \n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "        \n",
    "        g_error += G_loss\n",
    "        d_error += D_total_loss\n",
    "        \n",
    "        if index%15 ==0 :\n",
    "            vutils.save_image(real_images, '%s/real_samples.png' % \"./results\", normalize = True)\n",
    "            fake = generator((noise_vector,labels))\n",
    "            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"./results\", epoch), normalize = True)\n",
    "    print('Epoch {}: g_loss: {:.8f} d_loss: {:.8f}\\r'.format(epoch, g_error/index, d_error/index))\n",
    "    \n",
    "    img = generator((noise_vector,labels)).cpu().detach()\n",
    "    img = make_grid(img)\n",
    "    images.append(img)\n",
    "        \n",
    "    \n",
    "print('Training Finished')\n",
    "torch.save(generator.state_dict(), 'Conditional-GAN.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c9a4e9",
   "metadata": {},
   "source": [
    "## Looping through the Images and creating a GIF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a4d32703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/fake_samples_epoch_001.png\n",
      "./results/fake_samples_epoch_002.png\n",
      "./results/fake_samples_epoch_003.png\n",
      "./results/fake_samples_epoch_004.png\n",
      "./results/fake_samples_epoch_005.png\n",
      "./results/fake_samples_epoch_006.png\n",
      "./results/fake_samples_epoch_007.png\n",
      "./results/fake_samples_epoch_008.png\n",
      "./results/fake_samples_epoch_009.png\n",
      "./results/fake_samples_epoch_010.png\n",
      "./results/fake_samples_epoch_011.png\n",
      "./results/fake_samples_epoch_012.png\n",
      "./results/fake_samples_epoch_013.png\n",
      "./results/fake_samples_epoch_014.png\n",
      "./results/fake_samples_epoch_015.png\n",
      "./results/fake_samples_epoch_016.png\n",
      "./results/fake_samples_epoch_017.png\n",
      "./results/fake_samples_epoch_018.png\n",
      "./results/fake_samples_epoch_019.png\n",
      "./results/fake_samples_epoch_020.png\n",
      "./results/fake_samples_epoch_021.png\n",
      "./results/fake_samples_epoch_022.png\n",
      "./results/fake_samples_epoch_023.png\n",
      "./results/fake_samples_epoch_024.png\n",
      "./results/fake_samples_epoch_025.png\n",
      "./results/fake_samples_epoch_026.png\n",
      "./results/fake_samples_epoch_027.png\n",
      "./results/fake_samples_epoch_028.png\n",
      "./results/fake_samples_epoch_029.png\n",
      "./results/fake_samples_epoch_030.png\n",
      "./results/fake_samples_epoch_031.png\n",
      "./results/fake_samples_epoch_032.png\n",
      "./results/fake_samples_epoch_033.png\n",
      "./results/fake_samples_epoch_034.png\n",
      "./results/fake_samples_epoch_035.png\n",
      "./results/fake_samples_epoch_036.png\n",
      "./results/fake_samples_epoch_037.png\n",
      "./results/fake_samples_epoch_038.png\n",
      "./results/fake_samples_epoch_039.png\n",
      "./results/fake_samples_epoch_040.png\n",
      "./results/fake_samples_epoch_041.png\n",
      "./results/fake_samples_epoch_042.png\n",
      "./results/fake_samples_epoch_043.png\n",
      "./results/fake_samples_epoch_044.png\n",
      "./results/fake_samples_epoch_045.png\n",
      "./results/fake_samples_epoch_046.png\n",
      "./results/fake_samples_epoch_047.png\n",
      "./results/fake_samples_epoch_048.png\n",
      "./results/fake_samples_epoch_049.png\n",
      "./results/fake_samples_epoch_050.png\n",
      "./results/fake_samples_epoch_051.png\n",
      "./results/fake_samples_epoch_052.png\n",
      "./results/fake_samples_epoch_053.png\n",
      "./results/fake_samples_epoch_054.png\n",
      "./results/fake_samples_epoch_055.png\n",
      "./results/fake_samples_epoch_056.png\n",
      "./results/fake_samples_epoch_057.png\n",
      "./results/fake_samples_epoch_058.png\n",
      "./results/fake_samples_epoch_059.png\n",
      "./results/fake_samples_epoch_060.png\n",
      "./results/fake_samples_epoch_061.png\n",
      "./results/fake_samples_epoch_062.png\n",
      "./results/fake_samples_epoch_063.png\n",
      "./results/fake_samples_epoch_064.png\n",
      "./results/fake_samples_epoch_065.png\n",
      "./results/fake_samples_epoch_066.png\n",
      "./results/fake_samples_epoch_067.png\n",
      "./results/fake_samples_epoch_068.png\n",
      "./results/fake_samples_epoch_069.png\n",
      "./results/fake_samples_epoch_070.png\n",
      "./results/fake_samples_epoch_071.png\n",
      "./results/fake_samples_epoch_072.png\n",
      "./results/fake_samples_epoch_073.png\n",
      "./results/fake_samples_epoch_074.png\n",
      "./results/fake_samples_epoch_075.png\n",
      "./results/fake_samples_epoch_076.png\n",
      "./results/fake_samples_epoch_077.png\n",
      "./results/fake_samples_epoch_078.png\n",
      "./results/fake_samples_epoch_079.png\n",
      "./results/fake_samples_epoch_080.png\n",
      "./results/fake_samples_epoch_081.png\n",
      "./results/fake_samples_epoch_082.png\n",
      "./results/fake_samples_epoch_083.png\n",
      "./results/fake_samples_epoch_084.png\n",
      "./results/fake_samples_epoch_085.png\n",
      "./results/fake_samples_epoch_086.png\n",
      "./results/fake_samples_epoch_087.png\n",
      "./results/fake_samples_epoch_088.png\n",
      "./results/fake_samples_epoch_089.png\n",
      "./results/fake_samples_epoch_090.png\n",
      "./results/fake_samples_epoch_091.png\n",
      "./results/fake_samples_epoch_092.png\n",
      "./results/fake_samples_epoch_093.png\n",
      "./results/fake_samples_epoch_094.png\n",
      "./results/fake_samples_epoch_095.png\n",
      "./results/fake_samples_epoch_096.png\n",
      "./results/fake_samples_epoch_097.png\n",
      "./results/fake_samples_epoch_098.png\n",
      "./results/fake_samples_epoch_099.png\n",
      "./results/fake_samples_epoch_100.png\n",
      "./results/fake_samples_epoch_101.png\n",
      "./results/fake_samples_epoch_102.png\n",
      "./results/fake_samples_epoch_103.png\n",
      "./results/fake_samples_epoch_104.png\n",
      "./results/fake_samples_epoch_105.png\n",
      "./results/fake_samples_epoch_106.png\n",
      "./results/fake_samples_epoch_107.png\n",
      "./results/fake_samples_epoch_108.png\n",
      "./results/fake_samples_epoch_109.png\n",
      "./results/fake_samples_epoch_110.png\n",
      "./results/fake_samples_epoch_111.png\n",
      "./results/fake_samples_epoch_112.png\n",
      "./results/fake_samples_epoch_113.png\n",
      "./results/fake_samples_epoch_114.png\n",
      "./results/fake_samples_epoch_115.png\n",
      "./results/fake_samples_epoch_116.png\n",
      "./results/fake_samples_epoch_117.png\n",
      "./results/fake_samples_epoch_118.png\n",
      "./results/fake_samples_epoch_119.png\n",
      "./results/fake_samples_epoch_120.png\n",
      "./results/fake_samples_epoch_121.png\n",
      "./results/fake_samples_epoch_122.png\n",
      "./results/fake_samples_epoch_123.png\n",
      "./results/fake_samples_epoch_124.png\n",
      "./results/fake_samples_epoch_125.png\n",
      "./results/fake_samples_epoch_126.png\n",
      "./results/fake_samples_epoch_127.png\n",
      "./results/fake_samples_epoch_128.png\n",
      "./results/fake_samples_epoch_129.png\n",
      "./results/fake_samples_epoch_130.png\n",
      "./results/fake_samples_epoch_131.png\n",
      "./results/fake_samples_epoch_132.png\n",
      "./results/fake_samples_epoch_133.png\n",
      "./results/fake_samples_epoch_134.png\n",
      "./results/fake_samples_epoch_135.png\n",
      "./results/fake_samples_epoch_136.png\n",
      "./results/fake_samples_epoch_137.png\n",
      "./results/fake_samples_epoch_138.png\n",
      "./results/fake_samples_epoch_139.png\n",
      "./results/fake_samples_epoch_140.png\n",
      "./results/fake_samples_epoch_141.png\n",
      "./results/fake_samples_epoch_142.png\n",
      "./results/fake_samples_epoch_143.png\n",
      "./results/fake_samples_epoch_144.png\n",
      "./results/fake_samples_epoch_145.png\n",
      "./results/fake_samples_epoch_146.png\n",
      "./results/fake_samples_epoch_147.png\n",
      "./results/fake_samples_epoch_148.png\n",
      "./results/fake_samples_epoch_149.png\n",
      "./results/fake_samples_epoch_150.png\n",
      "./results/fake_samples_epoch_151.png\n",
      "./results/fake_samples_epoch_152.png\n",
      "./results/fake_samples_epoch_153.png\n",
      "./results/fake_samples_epoch_154.png\n",
      "./results/fake_samples_epoch_155.png\n",
      "./results/fake_samples_epoch_156.png\n",
      "./results/fake_samples_epoch_157.png\n",
      "./results/fake_samples_epoch_158.png\n",
      "./results/fake_samples_epoch_159.png\n",
      "./results/fake_samples_epoch_160.png\n",
      "./results/fake_samples_epoch_161.png\n",
      "./results/fake_samples_epoch_162.png\n",
      "./results/fake_samples_epoch_163.png\n",
      "./results/fake_samples_epoch_164.png\n",
      "./results/fake_samples_epoch_165.png\n",
      "./results/fake_samples_epoch_166.png\n",
      "./results/fake_samples_epoch_167.png\n",
      "./results/fake_samples_epoch_168.png\n",
      "./results/fake_samples_epoch_169.png\n",
      "./results/fake_samples_epoch_170.png\n",
      "./results/fake_samples_epoch_171.png\n",
      "./results/fake_samples_epoch_172.png\n",
      "./results/fake_samples_epoch_173.png\n",
      "./results/fake_samples_epoch_174.png\n",
      "./results/fake_samples_epoch_175.png\n",
      "./results/fake_samples_epoch_176.png\n",
      "./results/fake_samples_epoch_177.png\n",
      "./results/fake_samples_epoch_178.png\n",
      "./results/fake_samples_epoch_179.png\n",
      "./results/fake_samples_epoch_180.png\n",
      "./results/fake_samples_epoch_181.png\n",
      "./results/fake_samples_epoch_182.png\n",
      "./results/fake_samples_epoch_183.png\n",
      "./results/fake_samples_epoch_184.png\n",
      "./results/fake_samples_epoch_185.png\n",
      "./results/fake_samples_epoch_186.png\n",
      "./results/fake_samples_epoch_187.png\n",
      "./results/fake_samples_epoch_188.png\n",
      "./results/fake_samples_epoch_189.png\n",
      "./results/fake_samples_epoch_190.png\n",
      "./results/fake_samples_epoch_191.png\n",
      "./results/fake_samples_epoch_192.png\n",
      "./results/fake_samples_epoch_193.png\n",
      "./results/fake_samples_epoch_194.png\n",
      "./results/fake_samples_epoch_195.png\n",
      "./results/fake_samples_epoch_196.png\n",
      "./results/fake_samples_epoch_197.png\n",
      "./results/fake_samples_epoch_198.png\n",
      "./results/fake_samples_epoch_199.png\n",
      "./results/fake_samples_epoch_200.png\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "for i in range(1,len(images)+1):\n",
    "    image = imageio.v2.imread(f'%s/fake_samples_epoch_%03d.png'% (\"./results\", i))\n",
    "    frames.append(image[:500])\n",
    "    print(f'%s/fake_samples_epoch_%03d.png'% (\"./results\", i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "810c77bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimsave('./progress.gif', # output gif\n",
    "                frames,          # array of input frames\n",
    "                fps = 7)         # optional: frames per second"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
